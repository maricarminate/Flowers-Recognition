"""
SCRIPT DE AVALIA√á√ÉO COMPLETA DO MODELO
Gera relat√≥rio detalhado com todas as m√©tricas de desempenho
"""

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow import keras
from sklearn.metrics import (
    classification_report, 
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)
import json
from datetime import datetime

print("=" * 70)
print("üìä AVALIA√á√ÉO COMPLETA DO MODELO DE FLORES")
print("=" * 70)

# Configura√ß√µes
IMG_SIZE = 224
BATCH_SIZE = 32
DIRETORIO_VALIDACAO = 'data/validation'

# Emojis para flores
EMOJI_MAP = {
    'daisy': 'üåº',
    'dandelion': 'üåª',
    'rose': 'üåπ',
    'sunflower': 'üåª',
    'tulip': 'üå∑'
}

print("\n[1/6] Carregando modelo treinado...")

# Carregar modelo
try:
    if os.path.exists('modelos/melhor_modelo.keras'):
        modelo = keras.models.load_model('modelos/melhor_modelo.keras')
        modelo_usado = 'melhor_modelo.keras'
    else:
        modelo = keras.models.load_model('modelos/classificador_flores_final.keras')
        modelo_usado = 'classificador_flores_final.keras'
    
    print(f"‚úÖ Modelo carregado: {modelo_usado}")
except Exception as e:
    print(f"‚ùå Erro ao carregar modelo: {e}")
    print("üí° Execute primeiro: python classificador.py")
    exit(1)

# Carregar classes
try:
    with open('modelos/classes.txt', 'r', encoding='utf-8') as f:
        class_names = [linha.strip() for linha in f.readlines()]
    print(f"‚úÖ Classes carregadas: {len(class_names)} classes")
except:
    print("‚ö†Ô∏è  Arquivo classes.txt n√£o encontrado, usando ordem padr√£o")
    class_names = ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']

print("\n[2/6] Carregando dados de valida√ß√£o...")

# Carregar dataset de valida√ß√£o
val_ds = keras.preprocessing.image_dataset_from_directory(
    DIRETORIO_VALIDACAO,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    shuffle=False  # Importante para manter ordem
)

print(f"‚úÖ Dados de valida√ß√£o carregados")

# Contar imagens por classe
print("\nüìä Distribui√ß√£o do dataset de valida√ß√£o:")
for classe in class_names:
    caminho = f'{DIRETORIO_VALIDACAO}/{classe}'
    if os.path.exists(caminho):
        num = len([f for f in os.listdir(caminho) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
        emoji = EMOJI_MAP.get(classe, 'üå∏')
        print(f"   {emoji} {classe:12} {num:4} imagens")

print("\n[3/6] Fazendo predi√ß√µes no conjunto de valida√ß√£o...")

# Fazer predi√ß√µes
y_true = []
y_pred = []

num_batches = len(val_ds)
for i, (images, labels) in enumerate(val_ds):
    if (i + 1) % 5 == 0:
        print(f"   Progresso: {i + 1}/{num_batches} batches processados...")
    
    predictions = modelo.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(predictions, axis=1))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

print(f"‚úÖ {len(y_true)} predi√ß√µes realizadas")

print("\n[4/6] Calculando m√©tricas de desempenho...")

# Calcular m√©tricas
accuracy = accuracy_score(y_true, y_pred)
precision_macro = precision_score(y_true, y_pred, average='macro')
precision_weighted = precision_score(y_true, y_pred, average='weighted')
recall_macro = recall_score(y_true, y_pred, average='macro')
recall_weighted = recall_score(y_true, y_pred, average='weighted')
f1_macro = f1_score(y_true, y_pred, average='macro')
f1_weighted = f1_score(y_true, y_pred, average='weighted')

print("‚úÖ M√©tricas calculadas")

print("\n[5/6] Gerando visualiza√ß√µes...")

# Criar figura com 3 subplots
fig = plt.figure(figsize=(20, 6))

# ==================== SUBPLOT 1: MATRIZ DE CONFUS√ÉO ====================
ax1 = plt.subplot(1, 3, 1)

# Calcular matriz de confus√£o
cm = confusion_matrix(y_true, y_pred)

# Plotar com seaborn
sns.heatmap(
    cm, 
    annot=True, 
    fmt='d', 
    cmap='Blues',
    xticklabels=[f"{EMOJI_MAP.get(c, 'üå∏')} {c.capitalize()}" for c in class_names],
    yticklabels=[f"{EMOJI_MAP.get(c, 'üå∏')} {c.capitalize()}" for c in class_names],
    cbar_kws={'label': 'N√∫mero de Predi√ß√µes'},
    ax=ax1
)

ax1.set_title('üìä Matriz de Confus√£o', fontsize=16, fontweight='bold', pad=20)
ax1.set_xlabel('Predi√ß√£o', fontsize=13, fontweight='bold')
ax1.set_ylabel('Verdadeiro', fontsize=13, fontweight='bold')
plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')

# ==================== SUBPLOT 2: M√âTRICAS POR CLASSE ====================
ax2 = plt.subplot(1, 3, 2)

# Calcular m√©tricas por classe
report = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)

classes_display = [f"{EMOJI_MAP.get(c, 'üå∏')} {c.capitalize()}" for c in class_names]
precision_per_class = [report[c]['precision'] * 100 for c in class_names]
recall_per_class = [report[c]['recall'] * 100 for c in class_names]
f1_per_class = [report[c]['f1-score'] * 100 for c in class_names]

x = np.arange(len(classes_display))
width = 0.25

bars1 = ax2.bar(x - width, precision_per_class, width, label='Precis√£o', color='#3498db', alpha=0.8)
bars2 = ax2.bar(x, recall_per_class, width, label='Recall', color='#2ecc71', alpha=0.8)
bars3 = ax2.bar(x + width, f1_per_class, width, label='F1-Score', color='#e74c3c', alpha=0.8)

ax2.set_xlabel('Classe', fontsize=13, fontweight='bold')
ax2.set_ylabel('Score (%)', fontsize=13, fontweight='bold')
ax2.set_title('üìà M√©tricas por Classe', fontsize=16, fontweight='bold', pad=20)
ax2.set_xticks(x)
ax2.set_xticklabels(classes_display, rotation=45, ha='right')
ax2.legend(fontsize=11)
ax2.set_ylim([0, 105])
ax2.grid(axis='y', alpha=0.3, linestyle='--')

# Adicionar valores nas barras
for bars in [bars1, bars2, bars3]:
    for bar in bars:
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.1f}%',
                ha='center', va='bottom', fontsize=9, fontweight='bold')

# ==================== SUBPLOT 3: M√âTRICAS GERAIS ====================
ax3 = plt.subplot(1, 3, 3)

# Dados das m√©tricas gerais
metricas_nomes = ['Acur√°cia', 'Precis√£o\n(Macro)', 'Precis√£o\n(Weighted)', 
                  'Recall\n(Macro)', 'Recall\n(Weighted)', 'F1-Score\n(Macro)', 'F1-Score\n(Weighted)']
metricas_valores = [
    accuracy * 100,
    precision_macro * 100,
    precision_weighted * 100,
    recall_macro * 100,
    recall_weighted * 100,
    f1_macro * 100,
    f1_weighted * 100
]

colors = ['#9b59b6', '#3498db', '#5dade2', '#2ecc71', '#58d68d', '#e74c3c', '#ec7063']
bars = ax3.barh(metricas_nomes, metricas_valores, color=colors, alpha=0.8, edgecolor='black')

ax3.set_xlabel('Score (%)', fontsize=13, fontweight='bold')
ax3.set_title('üéØ M√©tricas Gerais do Modelo', fontsize=16, fontweight='bold', pad=20)
ax3.set_xlim([0, 105])
ax3.grid(axis='x', alpha=0.3, linestyle='--')

# Adicionar valores
for i, (bar, valor) in enumerate(zip(bars, metricas_valores)):
    ax3.text(valor + 1, i, f'{valor:.2f}%', 
            va='center', fontweight='bold', fontsize=11)

plt.tight_layout()
plt.savefig('resultados/avaliacao_completa.png', dpi=300, bbox_inches='tight')
print("‚úÖ Visualiza√ß√µes salvas: resultados/avaliacao_completa.png")

print("\n[6/6] Gerando relat√≥rio detalhado...")

# Gerar relat√≥rio em texto
relatorio_texto = f"""
{'=' * 70}
üìä RELAT√ìRIO DE AVALIA√á√ÉO DO MODELO
{'=' * 70}

Data/Hora: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}
Modelo: {modelo_usado}

{'=' * 70}
üéØ M√âTRICAS GERAIS
{'=' * 70}

Acur√°cia Geral:           {accuracy * 100:.2f}%

Precis√£o (Macro Avg):     {precision_macro * 100:.2f}%
Precis√£o (Weighted Avg):  {precision_weighted * 100:.2f}%

Recall (Macro Avg):       {recall_macro * 100:.2f}%
Recall (Weighted Avg):    {recall_weighted * 100:.2f}%

F1-Score (Macro Avg):     {f1_macro * 100:.2f}%
F1-Score (Weighted Avg):  {f1_weighted * 100:.2f}%

{'=' * 70}
üìà M√âTRICAS POR CLASSE
{'=' * 70}

"""

for classe in class_names:
    emoji = EMOJI_MAP.get(classe, 'üå∏')
    relatorio_texto += f"\n{emoji} {classe.upper()}\n"
    relatorio_texto += f"   Precis√£o:  {report[classe]['precision'] * 100:6.2f}%\n"
    relatorio_texto += f"   Recall:    {report[classe]['recall'] * 100:6.2f}%\n"
    relatorio_texto += f"   F1-Score:  {report[classe]['f1-score'] * 100:6.2f}%\n"
    relatorio_texto += f"   Suporte:   {report[classe]['support']:6} imagens\n"

relatorio_texto += f"\n{'=' * 70}\n"
relatorio_texto += f"üìä TOTAL DE IMAGENS AVALIADAS: {len(y_true)}\n"
relatorio_texto += f"{'=' * 70}\n"

# Salvar relat√≥rio
with open('resultados/relatorio_avaliacao.txt', 'w', encoding='utf-8') as f:
    f.write(relatorio_texto)

print("‚úÖ Relat√≥rio salvo: resultados/relatorio_avaliacao.txt")

# Salvar m√©tricas em JSON
metricas_json = {
    'data_avaliacao': datetime.now().strftime('%d/%m/%Y %H:%M:%S'),
    'modelo': modelo_usado,
    'metricas_gerais': {
        'acuracia': float(accuracy),
        'precisao_macro': float(precision_macro),
        'precisao_weighted': float(precision_weighted),
        'recall_macro': float(recall_macro),
        'recall_weighted': float(recall_weighted),
        'f1_macro': float(f1_macro),
        'f1_weighted': float(f1_weighted)
    },
    'metricas_por_classe': {
        classe: {
            'precisao': float(report[classe]['precision']),
            'recall': float(report[classe]['recall']),
            'f1_score': float(report[classe]['f1-score']),
            'suporte': int(report[classe]['support'])
        }
        for classe in class_names
    },
    'total_imagens': int(len(y_true))
}

with open('resultados/metricas.json', 'w', encoding='utf-8') as f:
    json.dump(metricas_json, f, indent=4, ensure_ascii=False)

print("‚úÖ M√©tricas JSON salvas: resultados/metricas.json")

# Exibir relat√≥rio na tela
print("\n" + "=" * 70)
print("üéâ AVALIA√á√ÉO CONCLU√çDA!")
print("=" * 70)

print(relatorio_texto)

# An√°lise de desempenho
print("=" * 70)
print("üí° AN√ÅLISE DE DESEMPENHO")
print("=" * 70)

if accuracy >= 0.95:
    print("\n   üåü EXCELENTE! Modelo de alt√≠ssima qualidade!")
elif accuracy >= 0.90:
    print("\n   üëç MUITO BOM! Modelo pronto para produ√ß√£o!")
elif accuracy >= 0.85:
    print("\n   ‚úÖ BOM! Desempenho satisfat√≥rio!")
elif accuracy >= 0.75:
    print("\n   ‚ö†Ô∏è  RAZO√ÅVEL. Considere mais treinamento ou dados.")
else:
    print("\n   ‚ùå BAIXO. Modelo precisa de melhorias significativas.")

# Identificar melhor e pior classe
melhor_classe = max(class_names, key=lambda c: report[c]['f1-score'])
pior_classe = min(class_names, key=lambda c: report[c]['f1-score'])

emoji_melhor = EMOJI_MAP.get(melhor_classe, 'üå∏')
emoji_pior = EMOJI_MAP.get(pior_classe, 'üå∏')

print(f"\n   üèÜ Melhor classe: {emoji_melhor} {melhor_classe.capitalize()} "
      f"(F1: {report[melhor_classe]['f1-score']*100:.2f}%)")
print(f"   üìâ Pior classe: {emoji_pior} {pior_classe.capitalize()} "
      f"(F1: {report[pior_classe]['f1-score']*100:.2f}%)")

print("\n" + "=" * 70)
print("üìÅ ARQUIVOS GERADOS:")
print("=" * 70)
print("   ‚úÖ resultados/avaliacao_completa.png")
print("   ‚úÖ resultados/relatorio_avaliacao.txt")
print("   ‚úÖ resultados/metricas.json")
print("=" * 70)